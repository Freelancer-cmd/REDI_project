"""
Compute average score and standard deviation per prompt class,
grouped by system prompt variant if present, from the evaluations.json
generated by LLM_as_judge.py.
"""

import json
import os
import re
import statistics


def main():
    base_dir = os.path.dirname(__file__)
    eval_path = os.path.join(base_dir, "evaluations.json")
    with open(eval_path, "r") as f:
        data = json.load(f)

    # Group entries by system prompt if present, else treat as single group
    data_by_sp = {}
    for entry in data:
        sp = entry.get("systemPrompt")
        data_by_sp.setdefault(sp, []).append(entry)

    for sp, entries in data_by_sp.items():
        if sp is not None:
            print("System Prompt:")
            print(sp)
            print()

        scores_by_class = {}
        for entry in entries:
            conv_id = entry.get("conversation_id", "")
            cls = conv_id.rsplit("_", 1)[0]
            evaluation = entry.get("evaluation", "")
            match = re.search(r"Score:\s*(\d+)", evaluation)
            if not match:
                continue
            score = int(match.group(1))
            scores_by_class.setdefault(cls, []).append(score)

        print("Class, Count, Average, StdDev")
        for cls in sorted(scores_by_class):
            scores = scores_by_class[cls]
            avg = statistics.mean(scores)
            std = statistics.pstdev(scores)
            print(f"{cls}, {len(scores)}, {avg:.2f}, {std:.2f}")
        print()


if __name__ == "__main__":
    main()