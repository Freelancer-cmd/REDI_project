"""
Compute average score and standard deviation per prompt class
from the evaluations.json generated by LLM_as_judge.py.
"""

import json
import os
import re
import statistics


def main():
    base_dir = os.path.dirname(__file__)
    eval_path = os.path.join(base_dir, "evaluations.json")
    with open(eval_path, "r") as f:
        data = json.load(f)

    scores_by_class = {}
    for entry in data:
        conv_id = entry.get("conversation_id", "")
        cls = conv_id.rsplit("_", 1)[0]
        evaluation = entry.get("evaluation", "")
        match = re.search(r"Score:\s*(\d+)", evaluation)
        if not match:
            continue
        score = int(match.group(1))
        scores_by_class.setdefault(cls, []).append(score)

    print("Class, Count, Average, StdDev")
    for cls in sorted(scores_by_class):
        scores = scores_by_class[cls]
        avg = statistics.mean(scores)
        std = statistics.pstdev(scores)
        print(f"{cls}, {len(scores)}, {avg:.2f}, {std:.2f}")


if __name__ == "__main__":
    main()